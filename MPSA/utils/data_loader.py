# import ml_collections
import sys
from timm.data import Mixup
from torch.utils.data import DataLoader, RandomSampler, DistributedSampler, SequentialSampler
from torchvision import transforms
from torchvision.transforms import InterpolationMode
from settings.setup_functions import get_world_size
from utils.dataset import *


def build_transforms(config):
	resize = int(config.data.img_size / 0.75)
	normalized_info = normalized()
	if config.data.no_crop:
		train_base = [transforms.Resize(config.data.img_size, InterpolationMode.BICUBIC),
		              transforms.RandomHorizontalFlip()]
		test_base = [transforms.Resize(config.data.img_size, InterpolationMode.BICUBIC),
		             transforms.CenterCrop(config.data.img_size)]
	else:
		train_base = [transforms.Resize((config.data.resize, config.data.resize), InterpolationMode.BICUBIC),
		              transforms.RandomHorizontalFlip()]
		test_base = [transforms.Resize((config.data.resize, config.data.resize), InterpolationMode.BICUBIC),
		             transforms.CenterCrop(config.data.img_size)]
	to_tensor = [transforms.ToTensor(),
	             transforms.Normalize(normalized_info['standard'][:3],
	                                  normalized_info['standard'][3:])]

	if config.data.blur > 0:
		train_base += [
			transforms.RandomApply([transforms.GaussianBlur(kernel_size=(5, 5), sigma=(0.1, 5))], p=config.data.blur),
			transforms.RandomAdjustSharpness(sharpness_factor=1.5, p=config.data.blur)]
	if config.data.color > 0:
		train_base += [transforms.ColorJitter(config.data.color, config.data.color, config.data.color, config.data.hue)]
	if config.data.rotate > 0:
		train_base += [transforms.RandomRotation(config.data.rotate, InterpolationMode.BICUBIC)]
	if config.data.autoaug:
		train_base += [transforms.AutoAugment(interpolation=InterpolationMode.BICUBIC)]
	train_base += [transforms.RandomCrop(config.data.img_size, padding=config.data.padding)]

	train_transform = transforms.Compose([*train_base, *to_tensor])
	test_transform = transforms.Compose([*test_base, *to_tensor])
	return train_transform, test_transform


def build_loader(config):
	train_transform, test_transform = build_transforms(config)
	
	# æ¨ç†æ¨¡å¼æ™ºèƒ½æ£€æµ‹
	is_inference = detect_inference_mode(config)
	if is_inference:
		print(f"ğŸ” æ£€æµ‹åˆ°æ¨ç†æ¨¡å¼ï¼Œå°†åŠ è½½ç«èµ›æµ‹è¯•é›†")

	train_set, test_set, num_classes = None, None, None
	if config.data.dataset == 'cub':
		root = os.path.join(config.data.data_root, 'CUB_200_2011')
		print(root)
		train_set = CUB(root, True, train_transform)
		test_set = CUB(root, False, test_transform)
		num_classes = 200

	elif config.data.dataset == 'cars':
		root = os.path.join(config.data.data_root, 'cars')
		train_set = Cars(root, True, train_transform)
		test_set = Cars(root, False, test_transform)
		num_classes = 196

	elif config.data.dataset == 'dogs':
		root = os.path.join(config.data.data_root, 'Dogs')
		train_set = Dogs(root, True, train_transform)
		test_set = Dogs(root, False, test_transform)
		num_classes = 120

	elif config.data.dataset == 'air':
		root = config.data.data_root
		train_set = Aircraft(root, True, train_transform)
		test_set = Aircraft(root, False, test_transform)
		num_classes = 100

	elif config.data.dataset == 'nabirds':
		root = os.path.join(config.data.data_root, 'nabirds')
		train_set = NABirds(root, True, train_transform)
		test_set = NABirds(root, False, test_transform)
		num_classes = 555

	elif config.data.dataset == 'pet':
		root = os.path.join(config.data.data_root, 'pets')
		train_set = OxfordIIITPet(root, True, train_transform)
		test_set = OxfordIIITPet(root, False, test_transform)
		num_classes = 37

	elif config.data.dataset == 'flowers':
		root = os.path.join(config.data.data_root, 'flowers')
		train_set = OxfordFlowers(root, True, train_transform)
		test_set = OxfordFlowers(root, False, test_transform)
		num_classes = 102

	elif config.data.dataset == 'food':
		root = config.data.data_root
		train_set = Food101(root, True, train_transform)
		test_set = Food101(root, False, test_transform)
		num_classes = 101

	elif config.data.dataset == 'webfg496':
		root = config.data.data_root
		if is_inference:
			# æ¨ç†æ¨¡å¼ï¼šåŠ è½½çœŸæ­£çš„ç«èµ›æµ‹è¯•é›†
			train_set = None
			test_set = WebFG496(root, False, test_transform)  # train=False
			print(f"âœ… åŠ è½½WebFG496ç«èµ›æµ‹è¯•é›†: {len(test_set)} æ ·æœ¬")
		else:
			# è®­ç»ƒæ¨¡å¼ï¼šä¿æŒç°æœ‰éªŒè¯é›†åˆ’åˆ†é€»è¾‘
			val_split = getattr(config.data, 'val_split', 0.2)
			
			# è®­ç»ƒé›†ï¼ˆ80%çš„åŸè®­ç»ƒæ•°æ®ï¼‰
			train_set = WebFG496(root, True, train_transform, val_split=val_split)
			train_set._split_train_val()  # è°ƒç”¨åˆ’åˆ†æ–¹æ³•è·å–è®­ç»ƒé›†éƒ¨åˆ†
			
			# éªŒè¯é›†ï¼ˆ20%çš„åŸè®­ç»ƒæ•°æ®ï¼‰- ä½œä¸ºtest_setè¿”å›ä»¥å¤ç”¨åŸæœ‰éªŒè¯é€»è¾‘
			test_set = WebFG496(root, True, test_transform, val_split=val_split)
			test_set.is_validation = True  # æ ‡è®°ä¸ºéªŒè¯é›†
			test_set._split_train_val()   # é‡æ–°åˆ’åˆ†è·å–éªŒè¯é›†æ ·æœ¬
		
		num_classes = 496

	elif config.data.dataset == 'webfg400':
		root = config.data.data_root
		if is_inference:
			# æ¨ç†æ¨¡å¼ï¼šåŠ è½½çœŸæ­£çš„ç«èµ›æµ‹è¯•é›†
			train_set = None
			test_set = WebFG400(root, False, test_transform)  # train=False
			print(f"âœ… åŠ è½½WebFG400ç«èµ›æµ‹è¯•é›†: {len(test_set)} æ ·æœ¬")
		else:
			# è®­ç»ƒæ¨¡å¼ï¼šä¿æŒç°æœ‰éªŒè¯é›†åˆ’åˆ†é€»è¾‘ï¼ˆå¤ç”¨WebFG496é€»è¾‘ï¼‰
			val_split = getattr(config.data, 'val_split', 0.2)
			
			# è®­ç»ƒé›†ï¼ˆ80%çš„åŸè®­ç»ƒæ•°æ®ï¼‰
			train_set = WebFG400(root, True, train_transform, val_split=val_split)
			train_set._split_train_val()  # è°ƒç”¨åˆ’åˆ†æ–¹æ³•è·å–è®­ç»ƒé›†éƒ¨åˆ†
			
			# éªŒè¯é›†ï¼ˆ20%çš„åŸè®­ç»ƒæ•°æ®ï¼‰- ä½œä¸ºtest_setè¿”å›ä»¥å¤ç”¨åŸæœ‰éªŒè¯é€»è¾‘
			test_set = WebFG400(root, True, test_transform, val_split=val_split)
			test_set.is_validation = True  # æ ‡è®°ä¸ºéªŒè¯é›†
			test_set._split_train_val()   # é‡æ–°åˆ’åˆ†è·å–éªŒè¯é›†æ ·æœ¬
		
		num_classes = 400

	elif config.data.dataset == 'webinat5000':
		root = config.data.data_root
		if is_inference:
			# æ¨ç†æ¨¡å¼ï¼šåŠ è½½çœŸæ­£çš„ç«èµ›æµ‹è¯•é›†
			train_set = None
			test_set = WebiNat5000(root, False, test_transform)  # train=False
			print(f"âœ… åŠ è½½WebiNat5000ç«èµ›æµ‹è¯•é›†: {len(test_set)} æ ·æœ¬")
		else:
			# è®­ç»ƒæ¨¡å¼ï¼šä¿æŒç°æœ‰éªŒè¯é›†åˆ’åˆ†é€»è¾‘ï¼ˆå¤ç”¨WebiNat5089é•¿å°¾ç­–ç•¥ï¼‰
			val_split = getattr(config.data, 'val_split', 0.2)
			
			# è®­ç»ƒé›†
			train_set = WebiNat5000(root, True, train_transform, val_split=val_split)
			train_set._split_train_val()  # è°ƒç”¨åˆ’åˆ†æ–¹æ³•è·å–è®­ç»ƒé›†éƒ¨åˆ†
			
			# éªŒè¯é›† - ä½œä¸ºtest_setè¿”å›ä»¥å¤ç”¨åŸæœ‰éªŒè¯é€»è¾‘
			test_set = WebiNat5000(root, True, test_transform, val_split=val_split)
			test_set.is_validation = True  # æ ‡è®°ä¸ºéªŒè¯é›†
			test_set._split_train_val()   # é‡æ–°åˆ’åˆ†è·å–éªŒè¯é›†æ ·æœ¬
		
		num_classes = 5000

	elif config.data.dataset == 'webinat5089':
		root = config.data.data_root
		if is_inference:
			# æ¨ç†æ¨¡å¼ï¼šåŠ è½½çœŸæ­£çš„ç«èµ›æµ‹è¯•é›†  
			train_set = None
			test_set = WebiNat5089(root, False, test_transform)  # train=False
			print(f"âœ… åŠ è½½WebiNat5089ç«èµ›æµ‹è¯•é›†: {len(test_set)} æ ·æœ¬")
		else:
			# è®­ç»ƒæ¨¡å¼ï¼šä¿æŒç°æœ‰éªŒè¯é›†åˆ’åˆ†é€»è¾‘
			val_split = getattr(config.data, 'val_split', 0.2)
			
			# è®­ç»ƒé›†ï¼ˆ80%çš„åŸè®­ç»ƒæ•°æ®ï¼‰
			train_set = WebiNat5089(root, True, train_transform, val_split=val_split)
			train_set._split_train_val()  # è°ƒç”¨åˆ’åˆ†æ–¹æ³•è·å–è®­ç»ƒé›†éƒ¨åˆ†
			
			# éªŒè¯é›†ï¼ˆ20%çš„åŸè®­ç»ƒæ•°æ®ï¼‰- ä½œä¸ºtest_setè¿”å›ä»¥å¤ç”¨åŸæœ‰éªŒè¯é€»è¾‘
			test_set = WebiNat5089(root, True, test_transform, val_split=val_split)
			test_set.is_validation = True  # æ ‡è®°ä¸ºéªŒè¯é›†
			test_set._split_train_val()   # é‡æ–°åˆ’åˆ†è·å–éªŒè¯é›†æ ·æœ¬
		
		num_classes = 5089
	# é’ˆå¯¹H800+å¤§è§„æ¨¡æ•°æ®é›†ä¼˜åŒ–çš„workeré…ç½®
	# åŸºäº176æ ¸CPUçš„æœ€ä¼˜é…ç½®ï¼šGPUæ ¸å¿ƒæ•°æ¯”ä¾‹ + æ•°æ®å¢å¼ºå¤æ‚åº¦è€ƒè™‘
	num_workers = 32 if sys.platform != 'win32' else 0  # å¤§å¹…æå‡å¹¶å‘workeræ•°
	if config.local_rank == -1:
		train_sampler = RandomSampler(train_set) if train_set is not None else None
		test_sampler = SequentialSampler(test_set)
	else:
		train_sampler = DistributedSampler(train_set, num_replicas=get_world_size(),
		                                   rank=config.local_rank, shuffle=True) if train_set is not None else None
		test_sampler = DistributedSampler(test_set)
	
	train_loader = DataLoader(train_set, sampler=train_sampler, batch_size=config.data.batch_size,
	                          num_workers=num_workers, drop_last=True, pin_memory=True, 
	                          persistent_workers=True, prefetch_factor=4) if train_set is not None else None
	test_loader = DataLoader(test_set, sampler=test_sampler, batch_size=config.data.batch_size,
	                         num_workers=num_workers, shuffle=False, drop_last=False, pin_memory=True,
	                         persistent_workers=True, prefetch_factor=4)

	mixup_fn = None
	mixup_active = config.data.mixup > 0. or config.data.cutmix > 0.
	if mixup_active:
		mixup_fn = Mixup(
			mixup_alpha=config.data.mixup, cutmix_alpha=config.data.cutmix,
			label_smoothing=config.model.label_smooth, num_classes=num_classes)

	return train_loader, test_loader, num_classes, len(train_set) if train_set is not None else 0, len(test_set), mixup_fn


def normalized():
	normalized_info = dict()
	normalized_info['standard'] = (0.485, 0.456, 0.406, 0.229, 0.224, 0.225)
	return normalized_info


def detect_inference_mode(config):
	"""æ™ºèƒ½æ£€æµ‹æ¨ç†æ¨¡å¼"""
	# æ–¹å¼1ï¼šæ˜¾å¼inference_modeå‚æ•°
	if hasattr(config.misc, 'inference_mode') and config.misc.inference_mode:
		return True
	
	# æ–¹å¼2ï¼ševal_modeå‚æ•°ï¼ˆå‘åå…¼å®¹ï¼‰
	if hasattr(config.misc, 'eval_mode') and config.misc.eval_mode:
		return True
	
	# æ³¨é‡Šæ‰é”™è¯¯çš„ç¬¬3ç§æ£€æµ‹ï¼šresumeä¸ä¸ºç©ºä¸ä»£è¡¨æ¨ç†æ¨¡å¼
	# å› ä¸ºresumeä¹Ÿå¯ä»¥ç”¨äºä»æ£€æŸ¥ç‚¹ç»§ç»­è®­ç»ƒ
	# if hasattr(config.model, 'resume') and config.model.resume:
	#     return True
		
	return False
